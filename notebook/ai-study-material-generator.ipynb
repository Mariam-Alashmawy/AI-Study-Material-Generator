{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Environment Setup\nInstall all required libraries for embeddings, vector database, LLM, and API.\n","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu langchain langchain-community langchain-core pypdf sentence-transformers transformers==4.52.4 torch ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastapi uvicorn pyngrok streamlit requests PyPDF2 -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Models Initialization\nload the required models.","metadata":{}},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. RAG Engine & FastAPI","metadata":{}},{"cell_type":"code","source":"import uvicorn\nimport threading\nimport time\nimport socket\nimport torch\nimport re\nimport io\nfrom fastapi import FastAPI, UploadFile, File, HTTPException, Header\nfrom pyngrok import ngrok, conf\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\nfrom langchain.prompts import PromptTemplate\nfrom PyPDF2 import PdfReader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Configuration\nNGROK_TOKEN = \"ADD_YOUR_TOKEN_HERE\" \nAPI_KEY = \"secret123\"\n\napp = FastAPI()\n\ndef extract_text_from_pdf(file_stream):\n    reader = PdfReader(file_stream)\n    extracted_text = \"\"\n    for page in reader.pages:\n        extracted_text += page.extract_text() + \"\\n\"\n    return extracted_text\n\n# Setting Up the Schemas\nresponse_schemas = [\n    ResponseSchema(name=\"quiz_title\", description=\"A title for the study session\"),\n    ResponseSchema(name=\"questions\", description=\"A list of 5 questions. Each must have 'question_text', 'options' (list of 4 strings), and 'correct_answer' (This MUST be the EXACT string text from the options list, not a number).\"),\n    ResponseSchema(name=\"flashcards\", description=\"A list of 5 flashcards with 'front' and 'back'.\")\n]\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\nstudy_template = \"\"\"\nYou are an expert tutor. Create a quiz and flashcards based on the text.\nIMPORTANT: The 'correct_answer' field must contain the EXACT text string from the 'options' list. \nDo not use numbers or indexes for 'correct_answer'.\n\nRespond ONLY in JSON format:\n{format_instructions}\n\nText Content: \"{extracted_text}\"\n\"\"\"\nprompt_temp = PromptTemplate(template=study_template, input_variables=[\"extracted_text\", \"format_instructions\"])\n\ndef generate_text(prompt_text):\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n    # Increased max_length to accommodate both quiz and flashcards\n    outputs = model.generate(**inputs, max_length=2500, do_sample=True, temperature=0.7)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ndef extract_json_block(text):\n    pattern = r'```json\\s*(.*?)\\s*```'\n    matches = re.findall(pattern, text, re.DOTALL)\n    return f\"```json\\n{matches[-1]}\\n```\" if matches else text\n\n@app.post(\"/generate_study_material\")\nasync def generate_study_material(file: UploadFile = File(...), authorization: str = Header(None)):\n    if authorization != f\"Bearer {API_KEY}\":\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n    \n    try:\n        pdf_bytes = await file.read()\n        raw_text = extract_text_from_pdf(io.BytesIO(pdf_bytes))\n        \n        # 1. Chunking: Split text into manageable pieces\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        chunks = text_splitter.split_text(raw_text)\n        \n        # 2. Vector DB: Create a temporary searchable database in memory\n        vector_db = FAISS.from_texts(chunks, embeddings)\n        \n        # 3. Retrieval: Find the most \"information-dense\" chunks\n        relevant_docs = vector_db.similarity_search(\"key concepts, definitions, and main summaries\", k=4)\n        context_text = \"\\n\".join([doc.page_content for doc in relevant_docs])\n        \n        format_instructions = output_parser.get_format_instructions()\n        # 4. Generation: Pass only the relevant chunks to Mistral\n        full_prompt = prompt_temp.format(extracted_text=context_text, format_instructions=format_instructions)\n        response = generate_text(full_prompt)\n        \n        json_text = extract_json_block(response)\n        return output_parser.parse(json_text)\n    except Exception as e:\n        return {\"error\": str(e)}\n\ndef find_port():\n    s = socket.socket(); s.bind(('', 0)); p = s.getsockname()[1]; s.close()\n    return p\n\napi_port = find_port()\nthreading.Thread(target=lambda: uvicorn.run(app, host=\"127.0.0.1\", port=api_port), daemon=True).start()\ntime.sleep(5)\nprint(f\"‚úÖ Study API ready on port {api_port}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Streamlit","metadata":{}},{"cell_type":"code","source":"streamlit_code = f\"\"\"\nimport streamlit as st\nimport requests\n\nst.set_page_config(page_title=\"AI Study Material Generator\", page_icon=\"üéì\")\nst.title(\"üéì AI Study Material Generator\")\n\nAPI_URL = \"http://127.0.0.1:{api_port}/generate_study_material\"\nAPI_KEY = \"secret123\"\n\nif \"study_data\" not in st.session_state:\n    st.session_state.study_data = None\n\nuploaded_file = st.file_uploader(\"Upload a PDF\", type=[\"pdf\"])\n\nif st.button(\"Generate Study Materials\"):\n    if uploaded_file:\n        with st.spinner(\"Generating...\"):\n            try:\n                files = {{\"file\": (uploaded_file.name, uploaded_file.getvalue(), \"application/pdf\")}}\n                headers = {{\"Authorization\": f\"Bearer {{API_KEY}}\"}}\\n\n                resp = requests.post(API_URL, files=files, headers=headers)\n                if resp.status_code == 200:\n                    st.session_state.study_data = resp.json()\n                else:\n                    st.error(\"API Error. Check your FastAPI console.\")\n            except Exception as e:\n                st.error(f\"Connection error: {{e}}\")\n\nif st.session_state.study_data:\n    data = st.session_state.study_data\n    tab1, tab2 = st.tabs([\"üìù Quiz\", \"üóÇÔ∏è Flashcards\"])\n    \n    with tab1:\n        with st.form(\"quiz_form\"):\n            user_answers = []\n            questions = data.get(\"questions\", [])\n            for i, q in enumerate(questions):\n                st.write(f\"**Q{{i+1}}:** {{q['question_text']}}\")\n                opts = [str(o) for o in q['options']]\n                \n                # FIX: Set index=None so no radio button is selected by default\n                ans = st.radio(f\"Select answer for Q{{i+1}}\", opts, index=None, key=f\"radio_{{i}}\")\n                \n                user_answers.append((ans, str(q['correct_answer'])))\n                st.divider()\n            \n            submitted = st.form_submit_button(\"Submit Quiz\")\n\n        if submitted:\n            score = 0\n            for i, (ua, ca) in enumerate(user_answers):\n                if ua is None:\n                    st.warning(f\"Q{{i+1}}: No answer selected.\")\n                elif ua.strip().lower() == ca.strip().lower():\n                    st.success(f\"Q{{i+1}}: Correct!\")\n                    score += 1\n                else:\n                    st.error(f\"Q{{i+1}}: Incorrect. You chose '{{ua}}', but the answer is '{{ca}}'\")\n            st.metric(\"Final Score\", f\"{{score}}/{{len(questions)}}\")\n\n    with tab2:\n        for card in data.get(\"flashcards\", []):\n            with st.container(border=True):\n                st.subheader(f\"Front: {{card['front']}}\")\n                with st.expander(\"Reveal Back\"):\n                    st.write(card['back'])\n\n    # The sidebar button has been removed from here.\n\"\"\"\nwith open(\"streamlit_app.py\", \"w\") as f:\n    f.write(streamlit_code)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Deployment via Ngrok","metadata":{}},{"cell_type":"code","source":"from pyngrok import ngrok, conf\nimport subprocess\nimport sys\n\nNGROK_TOKEN = \"39dFzgCkux5odvLlGAoolLxkCem_7mVjpzUCBJeFcKeqp8Kku\" \nconf.get_default().auth_token = NGROK_TOKEN\n\ntry:\n    for t in ngrok.get_tunnels(): ngrok.disconnect(t.public_url)\n    url = ngrok.connect(8501).public_url\n    print(f\"‚úÖ Access your Quiz Generator here: {url}\")\n    subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port\", \"8501\"])\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.system(\"pkill -f streamlit\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}